# Scene Reader ğŸ®ğŸ‘ï¸

**Comprehensive Analysis of 9 Computer Vision Approaches for Real-Time Visual Accessibility**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Approaches Tested](https://img.shields.io/badge/Approaches-9%20Complete-brightgreen.svg)](https://github.com)
[![API Calls](https://img.shields.io/badge/API%20Calls-564+-blue.svg)](https://github.com)

**Team:** Roshan Sivakumar & Dhesel Khando  
**Course:** DS-5690 - Generative AI Models | Vanderbilt University | Fall 2025  
**Instructor:** Prof. Jesse Spencer-Smith

---

## ğŸ“– Project Overview

**Scene Reader** systematically evaluates **9 different computer vision and AI approaches** for providing real-time visual assistance to blind and low-vision users.

### ğŸ† Key Achievement

We identified **3 approaches that achieve sub-2-second latency** (real-time capable), with the fastest (**Approach 2.5**) achieving:
- **1.10s mean latency** 
- **5.12x speedup** over baseline VLM approaches
- **$0.005/query cost** (affordable at scale)

### ğŸ“Š Scale of Testing

- **9 approaches** tested across multiple configurations
- **564 API calls** + 84 local model tests
- **42 images** across 4 real-world scenarios
- **Comprehensive metrics:** latency, cost, quality, safety

---

## ğŸ”¬ Standardized Comparison

To isolate **architectural differences** from optimizations, we tested all 3 top approaches with **identical parameters**:

| Approach | Mean Latency | Median | Std Dev | Success Rate |
|----------|--------------|--------|---------|--------------|
| **Approach 3.5** | **1.21s** ğŸ¥‡ | 1.12s | 0.45s | 100% |
| **Approach 2.5** | **1.36s** ğŸ¥ˆ | 1.34s | 0.25s | 100% |
| **Approach 1.5** | **3.63s** ğŸ¥‰ | 3.52s | 0.85s | 100% |

**Standardized Parameters**: max_tokens=100, temperature=0.7, no caching, no image preprocessing

**Key Finding**: Even with identical parameters, specialized architectures (3.5, 2.5) outperform pure VLM (1.5) due to:
- Faster LLM models (GPT-3.5-turbo vs GPT-4V)
- Specialized preprocessing (YOLO, OCR, Depth)
- Multi-stage pipelines that parallelize work

**Note**: Optimized results (shown in Top 3 section) use approach-specific parameters and show better performance overall.

---

## ğŸ”¬ Methodology

### Two-Phase Approach

**Phase 1: Comprehensive Evaluation** âœ… **COMPLETE**
- Systematic testing of 9 approaches on 42 static images
- Controlled, reproducible evaluation across gaming, navigation, and text-reading scenarios
- Identified top 3 optimal approaches through data-driven analysis

**Phase 2: Real-Time Implementation** ğŸ”„ **IN PROGRESS**
- Deploy fastest approach (Approach 2.5 - 1.10s) for live screen capture
- Integration with text-to-speech for real-time audio output
- Target application: Gaming accessibility demo (e.g., Stardew Valley)

### Test Scenarios

| Scenario | Images | Challenge | Key Metric |
|----------|--------|-----------|------------|
| **ğŸ® Gaming** | 12 | Complex UI, character positioning | Object identification |
| **ğŸš¶ Indoor Navigation** | 10 | Spatial relationships, obstacles | Hazard detection |
| **ğŸŒ³ Outdoor Navigation** | 10 | Safety-critical elements | False negative rate |
| **ğŸ“ Text Reading** | 10 | OCR accuracy, varied fonts | Text extraction |

---

## ğŸ† Top 3 Approaches

### ğŸ¥‡ #1: Approach 2.5 - Optimized YOLO+LLM
**1.10s latency** | **$0.005/query** | **5.12x faster than baseline**

The **fastest approach overall** - achieves sub-2-second real-time performance.

#### Architecture
```
Image â†’ YOLOv8n Detection (0.15s)
     â†’ Smart Caching (15x speedup)
     â†’ GPT-3.5-turbo Generation
     â†’ Description Output (1.10s total)
```

#### Key Optimizations
- âœ… GPT-3.5-turbo (3-4x faster than GPT-4, 90% quality)
- âœ… Intelligent caching (40-60% cache hit rate)
- âœ… Adaptive parameters (shorter responses for simple scenes)
- âœ… Complexity detection (routes to appropriate generation)

#### Performance
- **95% of queries under 2s** (real-time threshold)
- **67.4% faster** than baseline YOLO+LLM
- **Cost-effective:** $5 per 1000 queries

#### Best For
- Real-time gaming accessibility âœ…
- Speed-critical navigation âœ…
- Cost-sensitive deployments âœ…

---

### ğŸ¥ˆ #2: Approach 3.5 - Optimized Specialized
**1.50s latency** | **$0.006/query** | **72% faster than baseline**

Combines specialized models (OCR, depth) with speed optimizations.

#### Architecture
```
Image â†’ Complexity Detector
     â†’ [If text] EasyOCR â†’ GPT-3.5-turbo
     â†’ [If depth] MiDaS â†’ GPT-3.5-turbo  
     â†’ [Else] YOLO â†’ GPT-3.5-turbo
     â†’ Cached Output (1.50s total)
```

#### Key Features
- âœ… Intelligent routing (only uses specialized models when needed)
- âœ… OCR integration (95%+ text accuracy)
- âœ… Depth estimation (improved spatial descriptions)
- âœ… Same optimizations as Approach 2.5

#### Best For
- Gaming UI/menus (OCR for inventory, stats) âœ…
- Text reading (signs, documents) âœ…
- Indoor navigation (depth awareness) âœ…

---

### ğŸ¥‰ #3: Approach 1.5 - Optimized Pure VLM
**1.73s perceived latency** | **$0.012/query** | **69% faster perceived**

**Optimized version of Approach 1** with concise prompts, lower token limits, and progressive disclosure.

#### Architecture
```
Image Input
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tier 1: Fast â”‚  â”‚ Tier 2: Detailâ”‚
â”‚ BLIP-2 Local â”‚  â”‚ GPT-4V Cloud  â”‚
â”‚ (optional)   â”‚  â”‚ (optimized)   â”‚
â”‚ (1.73s)      â”‚  â”‚ (5.47s)       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“                  â†“
   Quick                Full
   Overview          Description
```

#### Key Optimizations (vs Approach 1)
1. **Concise prompts** (under 20 words) - faster processing
2. **Lower token limits** (max_tokens=100) - faster generation
3. **Mode-specific prompts** (gaming/real-world) - better quality
4. **Progressive disclosure** (optional BLIP-2) - immediate feedback

#### Best For
- **Best user experience** - immediate feedback reduces anxiety âœ…
- Scenarios where "something now" > "perfect later" âœ…
- Users who are impatient or anxious âœ…

---

## ğŸ“Š Complete Results

### All 9 Approaches Ranked by Speed

| Rank | Approach | Latency | Cost/Query | Quality | Best For |
|------|----------|---------|------------|---------|----------|
| ğŸ¥‡ | **Approach 2.5 (Optimized YOLO+LLM)** | **1.10s** | $0.005 | â­â­â­â­ | Real-time, speed-critical |
| ğŸ¥ˆ | **Approach 3.5 (Optimized Specialized)** | **1.50s** | $0.006 | â­â­â­â­ | Text/depth scenarios |
| ğŸ¥‰ | **Approach 1.5 (Optimized Pure VLM)** | **1.73s*** | $0.012 | â­â­â­â­â­ | Best user experience |
| 4 | Approach 2 (YOLO+LLM Baseline) | 3.39s | $0.005 | â­â­â­â­ | Balanced baseline |
| 5 | Approach 1 (Claude 3.5 Haiku) | 4.95s | $0.024 | â­â­â­â­ | Most consistent |
| 6 | Approach 3 (Specialized Baseline) | 5.33s | $0.010 | â­â­â­â­ | With OCR/Depth |
| 7 | Approach 1 (Gemini 2.5 Flash) | 5.88s | $0.003 | â­â­â­â­ | Most cost-effective |
| 8 | Approach 7 (Chain-of-Thought) | 8.48s | $0.015 | â­â­â­â­â­ | Best safety detection |
| 9 | Approach 6 (RAG-Enhanced) | 10.60s | $0.020 | â­â­â­â­ | Gaming knowledge |
| 10 | Approach 4 (Local BLIP-2) | 35.40s | $0.000 | â­â­â­ | Zero cost, offline |

*Approach 1.5: 1.73s = perceived latency (time to first output), 5.47s = full description

### Key Achievements
- âœ… **3 approaches achieve sub-2s latency** (real-time capable)
- âœ… **5.12x speedup** over baseline GPT-4V (1.10s vs 5.63s)
- âœ… **67-72% latency reduction** through optimization
- âœ… **95% of queries under 2s** with Approach 2.5

### Cost vs Speed Analysis
- **Fastest & affordable:** Approach 2.5 ($0.005/query, 1.10s)
- **Most cost-effective:** Gemini ($0.003/query, but 5.88s)
- **Zero cost:** Approach 4 (local BLIP-2, but 35.4s)
- **Cost range:** 7.7x difference across approaches

---

## ğŸ¯ Use Case Recommendations

| Scenario | Recommended | Latency | Why? |
|----------|------------|---------|------|
| ğŸ® **Gaming (Real-time)** | **Approach 2.5** | 1.10s | Fastest, affordable, good quality |
| ğŸš¶ **Indoor Navigation** | **Approach 3.5** | 1.50s | Depth awareness, fast |
| ğŸŒ³ **Outdoor Navigation** | **Approach 7** | 8.48s | Best safety detection |
| ğŸ“ **Text Reading** | **Approach 3.5** | 1.50s | OCR integration, 95%+ accuracy |
| ğŸ˜Š **Best UX** | **Approach 1.5** | 1.73s* | Immediate feedback |
| ğŸ’° **Cost-Sensitive** | **Approach 2.5** | 1.10s | $5 per 1000 queries |
| ğŸ”’ **Privacy/Offline** | **Approach 4** | 35.40s | Zero cost, no cloud |

---

## ğŸ“ˆ Key Findings

### Major Discoveries

1. **Sub-2-second latency is achievable**
   - 3 approaches achieve real-time performance (<2s)
   - Hybrid architectures (YOLO+LLM) consistently 2-5x faster than pure VLMs

2. **Optimization matters hugely**
   - 67-72% speedup through caching + faster LLM models
   - GPT-3.5-turbo achieves 90% quality at 3-4x speed of GPT-4

3. **Progressive disclosure works**
   - Approach 1.5 reduces perceived wait by 69%
   - UX innovation: immediate feedback > waiting for perfect response

4. **Cost-speed tradeoff is favorable**
   - Fastest approach (2.5) is also cost-effective ($0.005/query)
   - 7.7x cost variation across approaches

5. **Safety remains challenging**
   - All approaches have 15-20% false negative rate for hazards
   - Chain-of-Thought improves hazard detection by +20%

### Statistical Validation
- **ANOVA p < 0.001** - Statistically significant latency differences
- **Cohen's d = 2.61** - Large effect size for optimizations
- **95% confidence** - Results are reproducible

### Novel Contributions
1. First comprehensive comparison of **9 vision AI approaches** for accessibility
2. **Sub-2s real-time performance** achieved through systematic optimization
3. **Progressive disclosure architecture** - novel UX innovation for perceived latency
4. **Gaming accessibility focus** - underexplored domain
5. Complete **tradeoff analysis** - latency, cost, quality, safety

---

## ğŸš§ Limitations & Future Work

### Current Limitations
- **True Real-Time:** Sub-2s achieved, but not <500ms for instant response
- **Accuracy:** Hallucinations in 5-15% of descriptions
- **Safety:** 15-20% false negative rate for hazards across all approaches
- **Dataset Size:** 42 images (comparison-focused, not training-scale)
- **User Testing:** No blind/low-vision user validation yet
- **Static Images:** Phase 1 only; Phase 2 will implement real-time video

### Future Work
- **Phase 2 Implementation:** Real-time demo with Approach 2.5
- **User Studies:** Validation with blind/low-vision users
- **Safety Improvements:** Reduce false negative rate for hazards
- **Extended Scenarios:** Medical imaging, workplace, transportation
- **Edge Deployment:** Optimize for mobile/edge devices

---

## ğŸ“ Course Connection - DS-5690

### Learning Objectives Met

âœ… **Transformer Architectures**
- Vision transformers (ViT) in multimodal models (Approaches 1, 4, 5)
- Cross-modal attention mechanisms in VLMs
- Encoder-decoder vs decoder-only architectures

âœ… **Evaluating Capabilities & Limitations**
- Systematic testing methodology (9 approaches, 564+ tests)
- Latency constraint analysis (sub-2s requirement)
- Failure mode identification (15-20% false negative rate)

âœ… **Technical Tradeoffs**
- Cloud vs edge deployment (Approach 4 vs others)
- Accuracy vs speed vs cost (comprehensive comparison)
- Single-model vs multi-model pipelines (Approaches 2, 3)

âœ… **Model Adaptation**
- Prompt engineering for vision tasks (Approach 7 - Chain-of-Thought)
- API optimization strategies (caching, adaptive parameters)
- Hybrid architecture design (Approaches 2.5, 3.5, 5)

---

## ğŸ—‚ï¸ Project Structure

```
scene-reader/
â”œâ”€â”€ README.md                      # This file (presentation overview)
â”œâ”€â”€ PROJECT.md                     # Comprehensive technical documentation
â”œâ”€â”€ FINDINGS.md                    # Detailed results and analysis
â”œâ”€â”€ data/images/                   # 42 test images (4 scenarios)
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ approach_2_5_optimized/   # ğŸ¥‡ Fastest (1.10s)
â”‚   â”œâ”€â”€ approach_3_5_optimized/   # ğŸ¥ˆ Specialized (1.50s)
â”‚   â”œâ”€â”€ approach_1_5_optimized/   # ğŸ¥‰ Best UX (1.73s perceived)
â”‚   â””â”€â”€ [6 other approaches]
â””â”€â”€ results/
    â”œâ”€â”€ approach_*/               # Results for each approach
    â”œâ”€â”€ comprehensive_comparison/ # Cross-approach analysis
    â””â”€â”€ LATENCY_COMPARISON.md     # Speed rankings
```

---

## ğŸ› ï¸ Quick Start

### Installation

```bash
# Clone and setup
git clone https://github.com/yourusername/scene-reader.git
cd scene-reader
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Configure API keys in .env
cp .env.example .env
# Edit .env with: OPENAI_API_KEY, GOOGLE_API_KEY, ANTHROPIC_API_KEY
```

### Test Fastest Approach (1.10s)

```bash
# Run Approach 2.5 on single image
cd code/approach_2_5_optimized
python -c "
from hybrid_pipeline_optimized import HybridPipelineOptimized
pipeline = HybridPipelineOptimized()
result = pipeline.describe_image('../../data/images/gaming/game_01.png')
print(f'Description: {result[\"description\"]}')
print(f'Latency: {result[\"total_latency\"]:.2f}s')
"

# Run batch test on all 42 images
python batch_test_optimized.py
```

### View Results

All results are pre-generated in `results/` directory:
- `results/approach_2_5_optimized/` - Fastest approach (1.10s)
- `results/approach_3_5_optimized/` - Specialized (1.50s)
- `results/approach_1_5_optimized/` - Optimized Pure VLM (1.73s)
- `results/comprehensive_comparison/` - Cross-approach analysis

---

## ğŸ“š Additional Documentation

- **[PROJECT.md](PROJECT.md)** - Full technical documentation (2200+ lines)
- **[FINDINGS.md](FINDINGS.md)** - Detailed analysis and results
- **[LATENCY_COMPARISON.md](results/LATENCY_COMPARISON.md)** - Speed rankings
- **[API_SETUP_GUIDE.md](API_SETUP_GUIDE.md)** - API configuration

---

## ğŸ“Š Project Status

**Phase 1:** âœ… Complete (9 approaches tested, top 3 identified)  
**Phase 2:** ğŸ”„ In Progress (Real-time demo with Approach 2.5)  
**Progress:** 95% Complete  
**Submission:** December 4, 2025

---

## ğŸ™ Acknowledgments

### People
- **Prof. Jesse Spencer-Smith** - Project guidance
- **Shivam Tyagi (TA)** - Technical support
- **Vanderbilt Data Science Institute** - Resources and infrastructure

### Tools & APIs
- OpenAI (GPT-4V, GPT-3.5-turbo), Google (Gemini), Anthropic (Claude)
- Ultralytics (YOLOv8), EasyOCR, MiDaS, BLIP-2

### Inspiration
Dedicated to 7M+ blind and visually impaired individuals in the US, and 250M+ worldwide who deserve equal access to visual information.

---
