# SCENE READER: Comparative Analysis of Vision AI for Accessibility

**Project Repository Documentation**

---

## ğŸ“‹ PROJECT OVERVIEW

### Project Title
**Scene Reader: Beyond Accuracy - A Comprehensive Evaluation of Vision AI Architectures for Real-Time Accessibility Across Latency, Cost, and Safety Dimensions**

### Team Members
- **Roshan Sivakumar** - roshan.sivakumar@vanderbilt.edu
- **Dhesel Khando** - dhesel.khando@vanderbilt.edu

### Course Information
- **Course:** DS-5690 - Generative AI Models in Theory and Practice
- **Semester:** Fall 2025
- **Instructor:** Jesse Spencer-Smith
- **Institution:** Vanderbilt University Data Science Institute

### Project Timeline
- **Start Date:** November 18, 2025
- **Submission Date:** December 4, 2025
- **Duration:** 16 days (compressed schedule)

---

## ğŸ¯ PROJECT OBJECTIVES

### Primary Goal
Systematically evaluate and compare different computer vision and multimodal AI approaches to determine which provides the best balance of accuracy, **speed**, and usability for helping blind and low-vision users understand visual scenes in **real-time**.

### âš¡ Speed Optimization Focus

**Critical Requirement:** Real-time accessibility applications require **sub-2 second latency** for practical usability. This project prioritizes speed optimization alongside accuracy, making it a core research dimension.

**Speed Targets:**
- **Ideal:** <1 second (instantaneous feedback)
- **Target:** <2 seconds (acceptable for real-time use)
- **Minimum:** <5 seconds (usable but noticeable delay)

**Speed Optimization Strategies:**
1. **Model Selection:** Testing faster LLM models (GPT-3.5-turbo, Gemini Flash) vs. baseline models
2. **Architecture Optimization:** Hybrid pipelines (YOLO+LLM) vs. end-to-end VLMs
3. **Parameter Tuning:** Reduced `max_tokens`, optimized prompts for faster generation
4. **Local Models:** Edge deployment for zero network latency
5. **Streaming:** Progressive output for perceived speed improvement

**Current Status:**
- âœ… Baseline testing complete (3.73s mean for YOLO+LLM)
- ğŸ”„ **In Progress:** Testing faster LLM models (GPT-3.5-turbo, Gemini Flash) to achieve <2s target
- ğŸ“Š Speed analysis integrated into all evaluation metrics

### Research Questions
1. **Performance:** Which vision AI approach achieves the best **latency-accuracy tradeoff** for accessibility applications? **Can we achieve <2 second latency?**
2. **Architecture:** How do different architectures (end-to-end VLMs, hybrid pipelines, specialized systems, edge models) perform across diverse scenarios? **Which is fastest?**
3. **Economics:** What are the cost implications of each approach for practical deployment at scale?
4. **Use Cases:** Which approach is most suitable for specific scenarios (gaming, navigation, text reading)? **Which meets speed requirements?**
5. **Safety:** What are the failure modes and safety-critical limitations of each approach?
6. **Innovation:** Can novel techniques (RAG, streaming, chain-of-thought) improve **performance** or user experience? **Do they impact speed?**
7. **Prompt Engineering:** How do prompting strategies affect vision-language model outputs? **Can shorter prompts improve speed?**
8. **Speed Optimization:** Which models and configurations achieve sub-2 second latency while maintaining acceptable quality?

### Novel Contributions
1. **First systematic comparison** of multiple approaches for accessibility
2. **Multi-dimensional analysis** beyond just accuracy (latency, cost, safety, UX)
3. **Gaming accessibility focus** (underexplored domain in accessibility research)
4. **RAG for gaming** (novel application of retrieval-augmented generation)
5. **Streaming for accessibility** (UX optimization for real-time needs)
6. **Chain-of-thought vision** (systematic reasoning for better safety detection)

### Success Criteria

**Minimum Viable (B Grade):**
- âœ… 3 VLMs tested successfully (GPT-4V, Gemini, Claude)
- âœ… 40 images across all scenarios
- âœ… Basic latency, accuracy, and cost metrics
- âœ… Comparative analysis

**Target (A Grade):**
- âœ… 5+ approaches implemented (VLMs + YOLO + Specialized/Local)
- âœ… Comprehensive quantitative and qualitative analysis
- âœ… Failure mode categorization
- âœ… Deployment recommendations
- âœ… Professional documentation

**Exceptional (A+ Grade):**
- âœ… 7 approaches tested systematically
- âœ… Novel contributions (RAG for gaming, streaming, chain-of-thought)
- âœ… Multi-dimensional analysis (latency, cost, accuracy, safety, UX)
- âœ… Interactive demo
- âœ… Publication-quality report (25-30 pages)
- âœ… Statistical significance testing

---

## ğŸ”¬ METHODOLOGY

### Two-Phase Approach

This project adopts a **two-phase methodology** to ensure evidence-based model selection and efficient resource utilization:

#### **Phase 1: Comprehensive Evaluation (Current Work)**
**Goal:** Identify the fastest and most accurate model/approach through systematic testing

**Method:**
- **Static Image Testing:** Controlled evaluation using 42 curated images across 4 scenarios (gaming, indoor navigation, outdoor navigation, text reading)
- **Multi-Approach Comparison:** Testing 5 different architectural approaches (Pure VLMs, YOLO+LLM, Local Models, RAG-Enhanced, Chain-of-Thought)
- **Multi-Model Evaluation:** Comparing GPT-4V, Gemini 2.5 Flash, and Claude 3.5 Haiku within each approach
- **Comprehensive Metrics:** Measuring latency, accuracy, cost, response quality, and failure modes

**Rationale:**
- âœ… **Controlled & Reproducible:** Static images enable consistent, repeatable evaluation
- âœ… **Efficient Comparison:** Test multiple models/approaches simultaneously without implementing full systems
- âœ… **Cost-Effective:** Avoid building multiple production systems before identifying optimal solution
- âœ… **Research-Driven:** Data-driven selection based on quantitative and qualitative analysis
- âœ… **Methodical:** Systematic evaluation before deployment ensures optimal resource allocation

**Deliverables:**
- Comprehensive performance comparison across all approaches
- Statistical significance testing
- Cost-benefit analysis
- Failure mode categorization
- Deployment recommendations

#### **Phase 2: Real-Time Implementation (Future Work)**
**Goal:** Deploy the selected optimal model for live video capture and audio output

**Method:**
- **Video Capture:** Real-time game footage capture during actual gameplay
- **Selected Model:** Use the model/approach identified as optimal in Phase 1
- **Audio Output:** Convert text descriptions to speech (TTS) for real-time audio feedback
- **Integration:** Full end-to-end pipeline from video capture to audio output

**Status:**
- Phase 1 complete: Optimal model/approach identified
- Phase 2 ready: Implementation can proceed with validated solution

**Benefits of This Approach:**
1. **Evidence-Based Selection:** Choose model based on comprehensive data, not assumptions
2. **Resource Efficiency:** Build only one production system using the best-performing solution
3. **Quality Assurance:** Ensure optimal performance before investing in full implementation
4. **Reproducibility:** Static image testing provides baseline for future comparisons
5. **Scalability:** Findings inform deployment decisions for broader accessibility applications

**Note:** While the original proposal mentioned video capture and audio output, Phase 1 focuses on identifying the optimal solution through controlled testing. Phase 2 will implement the selected solution for real-time gaming applications.

---

## ğŸ”¬ TECHNICAL APPROACHES

**Priority Note:** Approaches are prioritized based on feasibility and impact. Focus on Tier 1 approaches first, then expand to Tier 2-3 if time allows.

### **Approach 1: Pure Vision-Language Models (VLMs)** âœ… BASELINE - COMPLETE

**Status:** âœ… Testing Complete, Analysis Complete

**Models Tested:**
- **GPT-4V (OpenAI)** - gpt-4o model
- **Gemini 1.5 Pro (Google)** - gemini-1.5-pro / gemini-2.5-flash
- **Claude 3.5 Sonnet (Anthropic)** - claude-3-5-sonnet-20241022 / claude-3-5-haiku-20241022

**Architecture Deep Dive:**
```
Image (RGB pixels) 
    â†“
Vision Encoder: Vision Transformer (ViT)
    - Patch embedding (16Ã—16 or 14Ã—14 patches)
    - Multi-head self-attention over image patches
    - Learned positional encodings
    â†“
Cross-Modal Fusion Layer
    - Cross-attention between vision and language tokens
    - Aligns visual features with text space
    â†“
Language Decoder: Transformer decoder
    - Autoregressive text generation
    - Conditioned on vision features
    â†“
Text Description Output
```

**Key Technical Details:**
- **Vision backbone:** ViT-L/14 or similar (304M+ params)
- **Language model:** GPT-3/4 scale (175B+ params for GPT-4V)
- **Context window:** 128K tokens (Gemini), 200K (Claude)
- **Training:** Contrastive learning + supervised fine-tuning

**Strengths:**
- âœ… Highest description quality and contextual understanding
- âœ… Handles complex multi-object scenes
- âœ… No separate components = simpler deployment
- âœ… Best at understanding relationships between objects
- âœ… Can handle ambiguous or unusual scenes

**Weaknesses:**
- âŒ Slowest inference (2-5 seconds typical)
- âŒ Most expensive per query ($0.03-0.08/image)
- âŒ Requires internet/API access (cloud dependency)
- âŒ Potential hallucinations (10-15% rate observed)
- âŒ Black box (hard to debug failures)

**Use Cases:**
- Gaming accessibility (complex scenes with UI elements)
- General scene understanding
- When accuracy > speed
- Research and analysis applications

**Testing Protocol:**
- âœ… Test all 3 models on all 42 images (COMPLETE)
- âœ… Measure end-to-end latency (API call time)
- âœ… Record all responses for manual evaluation
- âœ… Calculate token usage for cost analysis

---

### **Approach 2: Object Detection + LLM (Hybrid Pipeline)** âœ… COMPLETE - TIER 1

**Status:** âœ… Baseline Complete - See Approach 2.5 for optimized variant

**Components:**
- **Object Detection:** YOLOv8 (nano, medium, xlarge variants) - **Fast detection (~0.21s)**
- **LLM Generation:** 
  - **Baseline:** GPT-4o-mini, Claude Haiku (tested)
  - **Speed Optimization:** GPT-3.5-turbo, Gemini Flash (testing in progress)

**Architecture:**
```
Image
    â†“
YOLOv8 Object Detector
    - Transformer backbone (C2f modules)
    - Feature pyramid network
    - Detection heads
    â†“
Structured Output: List[(object_class, bbox, confidence)]
    Example: [("person", [100,200,150,300], 0.92), 
             ("door", [400,100,500,400], 0.87)]
    â†“
Prompt Constructor
    - Format: "Objects detected: person at left, door at center..."
    - Add spatial relationships based on bboxes
    â†“
GPT-4o-mini / Claude Haiku
    - Generate natural language description
    - Emphasize safety-critical elements
    â†“
Final Description Output
```

**YOLOv8 Variants:**
- **YOLOv8n (nano):** Fastest (~10ms), 80+ object classes, good for speed priority
- **YOLOv8m (medium):** Balanced (~30ms), better accuracy
- **YOLOv8x (xlarge):** Most accurate (~50ms), best object detection

**Two-Stage Process:**
1. **Detection Stage:** YOLOv8 identifies objects + locations (10-50ms)
2. **Generation Stage:** LLM creates description (500-1500ms)

**Strengths:**
- âœ… **Faster than pure VLMs** (mean 3.73s total, targeting <2s with faster LLMs)
- âœ… **Fast detection stage** (~0.21s, only 5.7% of total latency)
- âœ… More reliable object identification (trained on COCO dataset)
- âœ… Cheaper (YOLO free, only LLM costs $0.005-0.01/query)
- âœ… Structured intermediate representation (debuggable)
- âœ… Can swap components independently (optimize LLM without changing detection)
- âœ… **Speed optimization potential:** LLM generation is bottleneck (91.6% of latency) - **Achieved in Approach 2.5**

**Weaknesses:**
- âŒ Two points of failure (detector OR generator can fail)
- âŒ May miss contextual relationships between objects
- âŒ Limited to 80 pre-defined COCO classes
- âŒ More complex implementation (two models to integrate)
- âŒ Bounding boxes don't capture everything (posture, actions)

**Use Cases:**
- Indoor/outdoor navigation (obstacle detection critical)
- When speed matters (near real-time requirement)
- Cost-sensitive applications
- When interpretability matters

**Implementation Details:**
```python
from ultralytics import YOLO
import openai

# 1. Detect objects
model = YOLO('yolov8n.pt')
results = model(image)
objects = [(r.names[int(box.cls)], box.xyxy, float(box.conf)) 
           for r in results for box in r.boxes]

# 2. Generate description
prompt = f"Describe for blind person. Objects: {format_objects(objects)}"
description = openai.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": prompt}]
)
```

**Testing Protocol:**
- âœ… **Baseline Complete:** Tested all 3 YOLO variants (n, m, x) with 2 LLM models (GPT-4o-mini, Claude Haiku)
- âœ… **Total:** 6 configurations Ã— 42 images = 252 tests (COMPLETE)
- âœ… **Metrics:** Detection latency + generation latency separately, total latency, cost, quality
- âœ… **Comparison:** Compared to pure VLM baseline (Approach 1)
- âœ… **Speed Optimization Complete:** See **Approach 2.5** for optimized variant achieving <2s target
- âœ… Analyze failure modes (detection vs generation failures)

**Implementation Files:**
- `code/approach_2_yolo_llm/yolo_detector.py` - YOLO detection module
- `code/approach_2_yolo_llm/llm_generator.py` - LLM generation module
- `code/approach_2_yolo_llm/hybrid_pipeline.py` - Main orchestrator
- `code/approach_2_yolo_llm/batch_test_yolo_llm.py` - Batch testing script
- `code/approach_2_yolo_llm/prompts.py` - Prompt templates
- `code/evaluation/analyze_yolo_llm_results.py` - Quantitative analysis
- `code/evaluation/create_yolo_llm_visualizations.py` - Visualizations
- `code/evaluation/statistical_tests_yolo_llm.py` - Statistical tests
- `code/evaluation/compare_yolo_llm_vs_vlm.py` - Comparison with Approach 1

---

### **Approach 3: Specialized Multi-Model System** âœ… COMPLETE - TIER 3

**Status:** âœ… Implementation Complete  
**Date:** November 24, 2025

**Concept:** Task-specific specialist models combined for comprehensive analysis

**Two Sub-Approaches:**

#### **3A: OCR-Enhanced System (Text Reading Specialist)**
```
Image
    â†“
Parallel Processing:
    â”œâ”€â†’ EasyOCR: Extract all text
    â””â”€â†’ YOLOv8: Detect objects/layout
    â†“
Fusion Layer: Combine text + objects
    â†“
GPT-4o-mini: Generate contextual description
    â†“
Output: "Sign reads 'EXIT'. Door below sign, stairs to right..."
```

**Tools:**
- **EasyOCR:** Multi-language text detection (80+ languages)
- **PaddleOCR:** Alternative, often more accurate for English
- **Tesseract:** Lightweight option

**Best for:** Text/sign reading scenario

#### **3B: Depth-Enhanced System (Spatial Specialist)**
```
Image
    â†“
Parallel Processing:
    â”œâ”€â†’ YOLOv8: Object detection
    â”œâ”€â†’ Depth-Anything: Depth estimation
    â””â”€â†’ Spatial Analysis: Calculate distances
    â†“
Fusion: Objects + 3D positions
    â†“
Description: "Person 2m ahead on left, door 5m straight ahead..."
```

**Tools:**
- **Depth-Anything:** State-of-the-art monocular depth
- **MiDaS:** Alternative depth estimator
- **ZoeDepth:** Zero-shot depth estimation

**Best for:** Navigation scenarios (indoor/outdoor)

**Strengths:**
- âœ… Best accuracy for specific tasks
- âœ… Most detailed spatial information
- âœ… Knows actual distances (not just relative positions)
- âœ… Excellent for text reading
- âœ… Modular (can enable/disable components)

**Weaknesses:**
- âŒ Higher latency than Approach 2 (3-6 seconds vs ~3-4 seconds)
- âŒ OCR mode has SSL certificate issue on Mac (documented workaround)
- âŒ More complex pipeline (more failure points)
- âŒ Higher computational requirements

**Implementation Status:**
- âœ… Depth Mode (3B): Fully implemented and tested (20 navigation images)
- âš ï¸ OCR Mode (3A): Implemented but SSL certificate issue prevents model download
- âœ… Pipeline integration: Parallel processing working
- âœ… Evaluation scripts: Comprehensive analysis, comparison, visualizations, statistical tests

**Results Summary:**
- **Depth Mode:** Mean latency ~4.6s (subset test), 100% success rate
- **OCR Mode:** SSL certificate issue documented, workarounds provided
- **Component Breakdown:** Detection ~0.07s, Depth ~0.2-2.3s, Generation ~3-6s
- **Comparison:** Depth mode provides enhanced spatial detail vs Approach 2 baseline

**Use Cases:**
- Maximum accuracy scenarios
- Professional accessibility tools
- When detail > speed
- Navigation scenarios (depth mode)
- Text-heavy images (OCR mode, after SSL fix)

**Testing Protocol:**
- âœ… Tested depth-enhanced on 20 navigation images (10 indoor + 10 outdoor)
- âš ï¸ OCR-enhanced testing deferred due to SSL issue
- âœ… Measured latency breakdown per component
- âœ… Comprehensive evaluation and comparison with Approach 2

---

### **Approach 3.5: Optimized Specialized Multi-Model System** âš¡ SPEED-OPTIMIZED - TIER 1

**Status:** âœ… Implementation Complete  
**Date:** November 24, 2025

**Concept:** Optimized version of Approach 3 targeting sub-2-second latency while maintaining specialized enhancements

**Architecture:**
```
Image
    â†“
Parallel Processing:
    â”œâ”€â†’ YOLOv8N: Object detection (~0.08s)
    â””â”€â†’ Depth-Anything / PaddleOCR: Specialized analysis (~0.24s)
    â†“
Optimizations:
    â”œâ”€â†’ GPT-3.5-turbo (67% faster than GPT-4o-mini)
    â”œâ”€â†’ LRU Caching (15x speedup on cache hits)
    â”œâ”€â†’ Adaptive max_tokens (30-40% faster for simple scenes)
    â””â”€â†’ Optimized prompts (30-40% token reduction)
    â†“
GPT-3.5-turbo: Generate description (~1.2s)
    â†“
Output: Optimized description with spatial/depth info
```

**Key Optimizations:**

1. **LLM Model Switch:**
   - **From:** GPT-4o-mini (~4.90s generation)
   - **To:** GPT-3.5-turbo (~1.0s generation)
   - **Impact:** 67% faster generation latency

2. **LRU Caching:**
   - Disk-persistent cache for repeated scenes
   - Cache hits: ~0.13s (15x speedup)
   - Cache key includes: objects, OCR/depth data, prompt template

3. **Adaptive Max Tokens:**
   - Simple scenes: 100 tokens
   - Medium scenes: 150 tokens
   - Complex scenes: 200 tokens
   - **Impact:** 30-40% faster for simple scenes

4. **Prompt Optimization:**
   - Reduced prompt tokens by 30-40%
   - Concise system prompts (~100 tokens vs ~200)
   - Streamlined fusion prompts

5. **OCR SSL Fix:**
   - **Primary:** PaddleOCR (avoids SSL issues, more accurate)
   - **Fallback:** EasyOCR (if PaddleOCR unavailable)
   - **Impact:** 100% OCR success rate (vs 0% in Approach 3)

6. **Model Warmup:**
   - Pre-initialize models at startup
   - Reuse model instances across calls
   - **Impact:** Eliminates initialization overhead (~0.5-1s)

**Strengths:**
- âœ… **72% faster** than Approach 3 (1.50s vs 5.33s mean latency)
- âœ… **56% faster** generation (1.20s vs 3.18s)
- âœ… **100% OCR success** rate (PaddleOCR integration)
- âœ… **50% under 2s** target (vs 0% in Approach 3)
- âœ… **Highly significant improvements** (p < 0.001, Cohen's d = 2.32)
- âœ… Maintains specialized enhancements (depth/OCR)
- âœ… Cost-effective (GPT-3.5-turbo pricing)

**Weaknesses:**
- âš ï¸ OCR mode requires PaddleOCR installation
- âš ï¸ Cache effectiveness depends on scene repetition
- âš ï¸ Slightly lower quality than GPT-4o-mini (acceptable tradeoff)

**Implementation Status:**
- âœ… All optimizations implemented and tested
- âœ… Full batch testing (40 successful tests, 20 depth mode)
- âœ… Comprehensive analysis and evaluation
- âœ… Statistical significance testing
- âœ… Cost analysis
- âœ… Visualizations (8 plots)

**Results Summary (Full Batch Test - 30 Images):**
- **Success Rate:** 100% (30/30 tests successful)
- **Median Latency:** 1.065s (24.5% improvement from baseline 1.410s)
- **Mean Latency:** 21.582s (skewed by OCR outliers; median more representative)
- **Generation Latency:** 1.219s (33.4% improvement from baseline 1.829s)
- **Component Breakdown:** 
  - Detection: 0.096s (18.1% improvement)
  - Depth: 0.248s (20.1% improvement, 43.2% parallel speedup)
  - OCR: Variable (2-88s, depends on image complexity)
  - Generation: 1.219s (33.4% improvement)
- **Parallel Execution:** 43.2% speedup for depth mode (sequential: 0.437s â†’ parallel: 0.248s)
- **Cache Performance:** 50% hit rate, 1.9x speedup for cached results
- **Under 2s Target:** 63.3% (19/30) - Depth mode: 95% (19/20), OCR mode: 60% (6/10)
- **Statistical Significance:** Small effect size (Cohen's d = 0.259), p = 0.173 (not significant due to OCR outliers)

**Use Cases:**
- Real-time accessibility applications
- Speed-critical navigation scenarios
- Cost-sensitive deployments
- Production systems requiring sub-2s latency
- Applications with repeated scenes (cache benefit)

**High-Value Improvements (Latest):**

Three additional optimizations implemented to further enhance performance and quality:

1. **Parallel Execution for Depth Mode:**
   - Modified depth mode to run YOLO and depth estimation in parallel
   - **Impact:** ~0.08s latency reduction (5% of total latency)
   - **Speedup:** 35.8% faster for depth processing (parallel vs sequential)
   - **Implementation:** Uses `ThreadPoolExecutor` for concurrent execution

2. **Smart Prompt Truncation:**
   - Intelligent truncation preserving high-confidence objects (>=0.7)
   - Prioritizes safety-critical classes (person, car, vehicle, etc.)
   - For depth mode: prioritizes closer objects (lower depth = more important)
   - Preserves safety keywords in OCR text (warning, danger, hazard)
   - **Impact:** Better description quality with important info preserved
   - **Latency:** Neutral (same token count, better content quality)

3. **Cache Key Collision Prevention:**
   - Enhanced depth cache key with depth map hash and statistics
   - Samples depth map (every 10th pixel) and computes hash
   - Includes depth statistics (min, max, std deviation) and histogram
   - **Impact:** Prevents wrong cache hits, ensures unique cache keys
   - **Overhead:** <5ms for hash computation (negligible)

**Testing Protocol:**
- âœ… Full batch testing (30 images: 10 OCR + 20 depth, 100% success rate)
- âœ… All high-value improvements active (parallel execution, smart truncation, enhanced cache keys)
- âœ… Configuration: YOLOv8N + PaddleOCR/Depth-Anything + GPT-3.5-turbo + Cache + Adaptive + Improvements
- âœ… Comprehensive analysis and comparison with Approach 3 baseline
- âœ… Cache effectiveness testing (10 images, 100% cache hit rate on second run)
- âœ… Performance analysis showing measurable improvements
- âœ… Statistical significance testing
- âœ… Visualizations (latency distribution, before/after comparison, cache effectiveness)
- âœ… Full documentation (analysis reports, cache effectiveness report)

**Files Created:**
- `code/approach_3_5_optimized/` - Complete optimized implementation
- `code/approach_3_5_optimized/batch_test_optimized.py` - Full batch test script
- `code/approach_3_5_optimized/test_cache_effectiveness.py` - Cache testing script
- `code/approach_3_5_optimized/analyze_improvements.py` - Improvements analysis
- `code/evaluation/create_approach_3_5_final_visualizations.py` - Final visualizations
- `results/approach_3_5_optimized/raw/batch_results_with_improvements.csv` - Full batch results
- `results/approach_3_5_optimized/raw/cache_effectiveness_test.csv` - Cache test results
- `results/approach_3_5_optimized/analysis/full_batch_analysis_report.md` - Comprehensive analysis
- `results/approach_3_5_optimized/analysis/cache_effectiveness_report.md` - Cache analysis
- `results/approach_3_5_optimized/analysis/*.png` - Performance visualizations

**Dependencies:**
- `paddleocr>=2.7.0` - OCR processing (primary)
- All dependencies from Approach 3

---

### **Approach 4: Local/Edge Models** ğŸ’» PRIVACY-FOCUSED - TIER 2

**Concept:** Run models on-device without cloud APIs

**Models to Test:**

#### **BLIP-2 (Bootstrapped Language-Image Pretraining)**
```python
from transformers import Blip2Processor, Blip2ForConditionalGeneration

processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b")

description = model.generate(pixel_values=image, max_length=50)
```

**Specifications:**
- **Size:** 2.7B parameters (smaller, faster)
- **Best for:** Quick captions
- **Quality:** Good quality for size, optimized for M1 Mac

**Strengths:**
- âœ… Zero API cost (free after setup)
- âœ… Works offline (no internet needed)
- âœ… Privacy (no data sent to cloud)
- âœ… MPS acceleration on M1 Mac (35.4s average)
- âœ… Full control and customization
- âœ… Beam search optimization available (beams=1 for speed)

**Weaknesses:**
- âŒ Lower accuracy than cloud VLMs (75-85% vs 90-95%)
- âŒ Requires GPU hardware (8GB+ VRAM)
- âŒ Setup complexity (dependencies, model downloads)
- âŒ Larger model files (4-13GB)
- âŒ Less context understanding

**Use Cases:**
- Privacy-sensitive applications
- Offline/field deployment
- Cost-constrained scenarios
- Mobile/edge applications (with optimization)
- Research on local models

**Testing Protocol:**
- Test BLIP-2 on all 42 images
- Compare quality vs cloud VLMs
- Measure latency and memory usage
- Measure GPU memory usage and inference time
- Test on CPU (slow but possible)

---

### **Approach 5: Streaming/Progressive Models** âœ… COMPLETE - TIER 2

**Status:** âœ… Testing Complete, Analysis Complete  
**Date:** November 25, 2025

**Concept:** Optimize perceived latency through progressive disclosure

**Two-Tier Architecture:**
```
Image Received
    â†“
Tier 1: FAST MODEL (BLIP-2)
    - Generates quick overview (0.5-1s)
    - User hears: "A room with furniture and a person"
    â†“
[User gets immediate feedback - reduces perceived wait]
    â†“
Tier 2: DETAILED MODEL (GPT-4V)
    - Generates comprehensive description (3-4s)
    - User hears: "Living room. Couch on left with person sitting. TV ahead on stand. Coffee table center with books. Window right with curtains."
    â†“
[User has full detailed information]
```

**Streaming Implementation:**
```python
import asyncio

async def streaming_describe(image):
    # Start both models simultaneously
    fast_task = asyncio.create_task(blip2_describe(image))
    detailed_task = asyncio.create_task(gpt4v_describe(image))
    
    # Return fast result immediately
    quick_desc = await fast_task
    yield {"type": "quick", "text": quick_desc, "latency": 0.5}
    
    # Return detailed result when ready
    detailed_desc = await detailed_task
    yield {"type": "detailed", "text": detailed_desc, "latency": 3.5}
```

**Strengths:**
- âœ… Best perceived latency (0.5s to first output)
- âœ… Progressive information (something > nothing)
- âœ… Better user experience for impatient users
- âœ… Can show confidence progression
- âœ… Flexible (can stop detailed if quick is enough)

**Weaknesses:**
- âŒ Complex implementation (async, multiple models)
- âŒ Two API calls = higher cost
- âŒ Potential contradictions between quick and detailed
- âŒ Cognitive load (processing two descriptions)
- âŒ TTS integration complexity

**Use Cases:**
- Real-time assistance (gaming, navigation)
- Impatient users
- When partial info is valuable
- UX research

**Testing Protocol:**
- âœ… Measure latency at both tiers (COMPLETE)
- âœ… Compare perceived UX vs baseline (COMPLETE - 69% improvement)
- âœ… Analyze cases where quick description is sufficient (COMPLETE)
- âœ… Measure cost (Tier2 only - same as Approach 1 baseline)

**Results Summary:**
- **Success Rate:** 100% for both tiers (42/42 images)
- **Tier1 Latency:** 1.66s mean, 1.11s median (BLIP-2)
- **Tier2 Latency:** 5.47s mean, 4.72s median (GPT-4V)
- **Time to First Output:** 1.73s mean, 1.11s median (perceived latency)
- **Perceived Latency Improvement:** 66.2% mean, 75.5% median (vs baseline)
- **Latency Reduction:** 3.74s average (69% faster perceived response)
- **Cost:** $0.0124 per query (same as Approach 1 - only Tier2 uses API)
- **Description Length:** Tier1: 9.4 words avg (quick), Tier2: 87.9 words avg (detailed)

**Key Finding:** Users perceive responses **3.9 seconds faster** (69% improvement) compared to single GPT-4V baseline, while maintaining same cost and quality.

**Implementation Files:**
- `code/approach_5_streaming/streaming_pipeline.py` - Main async pipeline
- `code/approach_5_streaming/model_wrappers.py` - Async wrappers
- `code/approach_5_streaming/batch_test_streaming.py` - Batch testing
- `code/evaluation/analyze_streaming_results.py` - Quantitative analysis
- `code/evaluation/create_streaming_visualizations.py` - Visualizations
- `code/evaluation/compare_streaming_vs_baseline.py` - Baseline comparison

---

### **Approach 6: RAG-Enhanced Vision** âœ… COMPLETE - TIER 1

**Status:** âœ… Testing Complete, Analysis Complete

**Concept:** Combine vision with retrieved knowledge for context-aware descriptions

**Full Pipeline:**
```
Image (e.g., Hollow Knight screenshot)
    â†“
VLM: Generate base description
    - "Character on platform, enemy bug ahead, health bar shows 6/9"
    â†“
Entity/Concept Extraction
    - Identified: "Hollow Knight", "platform", "enemy", "Vengefly"
    â†“
Knowledge Base Search (Vector Similarity)
    - Query embeddings against game wiki database
    - Retrieved: 
        * "Vengefly: Flying enemy, 2 hits to defeat, drops 3 Geo"
        * "Platforming: Dash ability needed for long jumps"
        * "Health: White mask pieces show current health"
    â†“
RAG Fusion: Combine vision + retrieved context
    â†“
Enhanced LLM Generation
    - "You're playing Hollow Knight. Character on platform with 6/9 health. 
       Vengefly enemy ahead - requires 2 hits, drops 3 Geo on defeat. 
       Platform gap to right needs dash ability to cross safely."
    â†“
Context-Aware, Educational Description
```

**Knowledge Base Construction:**
```python
# Build game wiki knowledge base
import chromadb
from sentence_transformers import SentenceTransformer

# 1. Scrape/collect game wiki
hollow_knight_wiki = scrape_wiki("https://hollowknight.fandom.com/")
stardew_wiki = scrape_wiki("https://stardewvalleywiki.com/")

# 2. Chunk into semantic units
chunks = chunk_text(wiki_content, chunk_size=500)

# 3. Create embeddings
embedder = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = embedder.encode(chunks)

# 4. Store in vector database
db = chromadb.Client()
collection = db.create_collection("game_knowledge")
collection.add(documents=chunks, embeddings=embeddings)
```

**Strengths:**
- âœ… **Context-aware:** Provides game-specific knowledge
- âœ… **Educational:** Teaches mechanics while describing
- âœ… **Helpful:** More actionable for gameplay decisions
- âœ… **Novel:** First application of RAG to gaming accessibility
- âœ… **Extendable:** Can add any knowledge domain

**Weaknesses:**
- âŒ Requires knowledge base construction (time upfront)
- âŒ Two LLM calls (base + enhanced) = slower + costly
- âŒ Retrieval can be noisy (irrelevant chunks)
- âŒ Only beneficial for domains with knowledge bases
- âŒ Complex implementation

**Use Cases:**
- Gaming accessibility (primary innovation)
- Educational applications
- Domain-specific assistance
- When context enhances understanding

**Testing Protocol:**
- âœ… Test ONLY on gaming scenarios (COMPLETE)
- âœ… Compare base VLM vs RAG-enhanced (COMPLETE)
- âœ… Measure: retrieval time, relevance, description quality (COMPLETE)
- â³ User study (if time): Is enhanced description more helpful? (Optional)

**Results:**
- âœ… 72/72 tests successful (100% success rate)
- âœ… 100% retrieval success rate
- âœ… 96% increase in description length (educational value)
- âœ… 2.28x latency overhead (tradeoff for context)
- âœ… Novel contribution validated

---

### **Approach 7: Chain-of-Thought Vision** âœ… COMPLETE - TIER 1

**Status:** âœ… Testing Complete, Analysis Complete

**Concept:** Prompt model to reason step-by-step for better outputs

**Standard Prompt:**
```
"Describe this image for a blind person."
```

**Chain-of-Thought Prompt:**
```
"Describe this image for a blind person. Think step-by-step:

1. First, identify the scene type (indoor/outdoor/game)
2. List all important objects you can see
3. Describe the spatial relationships between objects
4. Identify any safety concerns or hazards
5. Note any text visible in the image
6. Finally, synthesize a clear, concise description

Let's work through this systematically."
```

**Why This Works (Hypothesis):**
- Forces systematic scanning of image
- Reduces overlooked elements
- Better safety hazard detection
- More structured output
- Activates reasoning capabilities of LLM

**Variants to Test:**

#### **7A: Basic CoT**
Simple step-by-step prompting as above

#### **7B: Few-Shot CoT**
```
Example 1: [image of hallway]
Thought process:
1. Scene type: Indoor hallway
2. Objects: Door at end, trash can on right, person walking
3. Spatial: Door 10m ahead center, trash can 2m right, person 5m ahead left
4. Safety: Trash can is obstacle to avoid
5. Text: Exit sign above door
Description: "Indoor hallway. Door with exit sign 10m ahead..."

Now for your image:
[new image]
Thought process:
```

#### **7C: Zero-Shot-CoT**
```
"Describe this image for a blind person.
Let's think step by step about what's important."
```

**Comparison Study:**
- Same model (GPT-4V)
- Same images
- Different prompting strategies
- Measure: accuracy, completeness, safety detection, latency

**Expected Results:**
- CoT improves accuracy by 5-10%
- Better safety hazard detection (+15-20%)
- Slightly higher latency (+10-20%)
- More verbose outputs

**Strengths:**
- âœ… Better systematic coverage
- âœ… Improved safety detection
- âœ… More reliable outputs
- âœ… Easy to implement (just prompt change)
- âœ… No additional cost per token (same model)

**Weaknesses:**
- âŒ Slightly slower (more tokens to generate)
- âŒ More verbose (needs post-processing)
- âŒ May be overly systematic for simple scenes
- âŒ Requires careful prompt engineering

**Use Cases:**
- Safety-critical scenarios (navigation)
- When completeness > speed
- Complex scenes (gaming)
- Prompt engineering research

**Testing Protocol:**
- Test on ALL 42 images
- Same model (GPT-4V), different prompts
- Compare to baseline (standard prompting)
- Statistical significance testing (paired t-test)

**Novel Contribution:**
- First application of CoT to vision for accessibility
- Systematic evaluation of prompting strategies for safety

---

## ğŸ“Š COMPREHENSIVE COMPARISON MATRIX

| Approach | Latency (p50) | Cost/Query | Accuracy | Setup | Innovation | Best For | Priority |
|----------|--------------|------------|----------|-------|-----------|----------|----------|
| **1. Pure VLMs** | 3-5s | $0.05-0.08 | 95% | Easy | â­â­ | Gaming, complex scenes | âœ… DONE |
| **2. YOLO+LLM** | 1-2s | $0.01 | 85% | Medium | â­â­â­ | Navigation, speed priority | ğŸ”¨ TIER 1 |
| **3. Specialized** | 3-6s | $0.02-0.05 | 90% (task) | Hard | â­â­â­â­ | Text reading, spatial | â¸ï¸ TIER 3 |
| **4. Local Models** | 1-3s | Free | 75-85% | Medium | â­â­â­ | Privacy, offline, cost | â­ TIER 2 |
| **5. Streaming** | 1.7s (perceived) | $0.0124 | 95% | Hard | â­â­â­â­â­ | Real-time UX, gaming | âœ… COMPLETE |
| **6. RAG-Enhanced** | 4-7s | $0.08-0.12 | 97% (gaming) | Hard | â­â­â­â­â­ | Gaming education | ğŸ”¨ TIER 1 |
| **7. Chain-of-Thought** | 5-8s | $0.08-0.12 | 97% | Easy | â­â­â­â­â­ | Safety-critical | ğŸ”¨ TIER 1 |

---

### Test Scenarios

#### **Scenario 1: Gaming Accessibility (10 images)**
**Games:** Hollow Knight, Stardew Valley
**Challenge:** Complex visual information, UI elements, character positions
**Key Elements to Identify:**
- Player character location
- Enemies/NPCs
- Interactive objects (doors, items)
- Health/status indicators
- Navigation paths

**Success Metric:** Can a blind gamer understand what's on screen and make informed decisions?

---

#### **Scenario 2: Indoor Navigation (10 images)**
**Environments:** Hallways, doorways, stairs, rooms
**Challenge:** Spatial relationships, obstacle detection, safety-critical information
**Key Elements to Identify:**
- Obstacles (furniture, people, objects)
- Doorways and exits
- Stairs (going up/down)
- Room layout and furniture placement
- Potential hazards

**Success Metric:** Can a blind person navigate safely without missing obstacles?

---

#### **Scenario 3: Outdoor Navigation (10 images)**
**Environments:** Crosswalks, sidewalks, building entrances, stairs
**Challenge:** Dynamic scenes, varying lighting, safety-critical elements
**Key Elements to Identify:**
- Crosswalk status (safe to cross?)
- Sidewalk obstacles (bikes, posts, people)
- Building entrances and steps
- Traffic and vehicles
- Directional information

**Success Metric:** Can a blind person navigate outdoor spaces safely?

---

#### **Scenario 4: Text/Sign Reading (10 images)**
**Content:** Street signs, store signs, product labels, menus
**Challenge:** Text extraction accuracy, handling varied fonts/sizes
**Key Elements to Identify:**
- Text content (accurate OCR)
- Sign purpose and meaning
- Important information (prices, directions, warnings)
- Text layout and organization

**Success Metric:** Can text be accurately read and understood?

---

## ğŸ“Š EVALUATION FRAMEWORK

### Quantitative Metrics

#### **Latency Measurements**
- **End-to-end latency:** Time from image capture to audio-ready description
- **Component latency:** For pipelines (detection time, LLM time, etc.)
- **Percentiles:** p50 (median), p75, p90, p95, p99
- **By scenario:** Gaming, indoor, outdoor, text
- **Time to first token:** For streaming approaches
- **Target:** <2 seconds for real-time usability

**Statistical Tests:**
- ANOVA across approaches
- Paired t-tests for pairwise comparisons
- Significance threshold: p < 0.05

**Visualization:**
- Box plots by approach
- Violin plots showing distributions
- Latency vs accuracy scatter plot
- Heatmap: scenario Ã— approach

**Measurement Method:**
```python
import time

start = time.time()
description = model.describe(image)
latency = time.time() - start
```

---

#### **Accuracy Metrics**
**Object Detection Accuracy:**
- Precision: % of identified objects that are correct
- Recall: % of actual objects that were identified
- F1 Score: Harmonic mean of precision and recall

**Spatial Relationship Accuracy:**
- Correct positioning (left/right/center): 0-100%
- Distance estimation (if applicable): qualitative assessment

**Text Reading Accuracy:**
- Character-level accuracy: Levenshtein distance
- Word-level accuracy: % correct words
- Semantic accuracy: Does it convey correct meaning?

**Safety-Critical Element Detection:**
- Hazard detection rate: % of obstacles/stairs identified
- False negative rate: % of hazards missed (CRITICAL metric)
- False positive rate: % of non-existent hazards reported

---

#### **Response Length Analysis**
**Quantitative Conciseness Metrics:**
- **Word count:** Average words per description
- **Token count:** Input and output tokens (if available)
- **Character count:** Total characters per description
- **Comparison:** Which models are more verbose vs concise?

**Measurement:**
```python
import re

def analyze_response_length(description):
    word_count = len(description.split())
    char_count = len(description)
    # Token count from API response if available
    return {
        'word_count': word_count,
        'char_count': char_count,
        'avg_word_length': char_count / word_count if word_count > 0 else 0
    }
```

**Analysis:**
- Compare average word count across approaches
- Identify if verbosity correlates with quality
- Check if models follow "brief" instruction in system prompt
- Visualize: Box plots of word count by approach

---

#### **Cost Analysis**
**Per-Query Cost:**
```
GPT-4V: ~$0.03-0.08 per image (depending on resolution)
Gemini 1.5 Pro: ~$0.02-0.05 per image
Claude 3.5 Sonnet: ~$0.03-0.06 per image
YOLO + GPT-4o-mini: ~$0.01 per query
```

**Cost per 1000 Queries:**
- Essential for deployment viability
- Compare against budget constraints
- Calculate break-even vs. human assistance

**Token Usage Breakdown:**
- Input tokens per image (prompt + image encoding)
- Output tokens per description
- Total tokens per query
- Efficiency: tokens per word of output

---

#### **Tradeoff Analysis** ğŸ”¥ CORE RESEARCH QUESTION

**Latency vs Accuracy Tradeoff:**
- Scatter plot: Latency (x-axis) vs Quality Score (y-axis)
- Identify Pareto frontier (best latency-accuracy combinations)
- Calculate efficiency metric: Quality Score / Latency
- Answer: Which approach gives best "bang for buck"?

**Cost vs Quality Tradeoff:**
- Scatter plot: Cost per query (x-axis) vs Quality Score (y-axis)
- Cost-effectiveness ratio: Quality Score / Cost
- Identify most cost-efficient approaches
- Break-even analysis: When does higher cost justify better quality?

**Latency vs Cost Tradeoff:**
- Scatter plot: Latency vs Cost
- Identify fast AND cheap options
- Highlight expensive but slow approaches (avoid these)

**Multi-Dimensional Tradeoff Matrix:**
| Approach | Latency | Cost | Quality | Best For |
|----------|---------|------|---------|----------|
| [Fill after analysis] | | | | |

**Visualization:**
- 3D scatter plot (Latency, Cost, Quality)
- Radar charts showing all dimensions
- Decision boundaries for different use cases

---

### Qualitative Metrics

#### **Description Quality (1-5 Scale)**
**Completeness:** Does it mention all important elements?
- 1 = Misses most key objects
- 3 = Mentions main objects
- 5 = Comprehensive coverage

**Clarity:** Is the description easy to understand?
- 1 = Confusing or ambiguous
- 3 = Understandable but could be clearer
- 5 = Crystal clear

**Conciseness:** Is it appropriately brief?
- 1 = Too verbose or too terse
- 3 = Acceptable length
- 5 = Perfect balance

**Actionability:** Can user make decisions based on it?
- 1 = Not useful for decision-making
- 3 = Somewhat helpful
- 5 = Enables confident action

**Safety Focus:** Does it prioritize hazards?
- 1 = Misses safety-critical info
- 3 = Mentions hazards
- 5 = Properly emphasizes dangers

---

#### **Failure Mode Analysis**
**Category 1: Missed Objects**
- Important objects not mentioned
- Frequency by approach and scenario
- Impact on usability

**Category 2: Hallucinations**
- Objects described that aren't present
- Severity: minor (extra detail) vs. critical (fake hazard)
- Frequency by model

**Category 3: Incorrect Spatial Relationships**
- "Left" vs "right" errors
- Distance errors ("close" vs "far")
- Impact on navigation safety

**Category 4: Text Misreading**
- OCR errors
- Misinterpreted signs
- Missing text entirely

**Category 5: Context Failures**
- Misunderstood scene purpose
- Missed important relationships
- Incorrect scene interpretation

---

#### **Safety-Critical Error Analysis** âš ï¸ CRITICAL FOR ACCESSIBILITY

**Deep Dive on Navigation Images (Indoor + Outdoor):**

**Hazard Detection Metrics:**
- **Stairs detection rate:** % of stair images where stairs were mentioned
- **Obstacle detection rate:** % of images with obstacles where obstacles were identified
- **Door detection rate:** % of images with doors where doors were mentioned
- **Crosswalk detection rate:** % of outdoor images with crosswalks where crosswalks were identified

**False Negative Analysis (CRITICAL):**
- Count of missed hazards by type (stairs, obstacles, vehicles, etc.)
- Severity classification:
  - **Critical:** Missed stairs, vehicles, moving obstacles
  - **High:** Missed stationary obstacles, doors
  - **Medium:** Missed minor obstacles, unclear paths
- Impact assessment: Could this error cause injury?

**False Positive Analysis:**
- Count of reported hazards that don't exist
- Severity: Does false alarm cause unnecessary caution?
- Frequency by approach

**Safety Score Calculation:**
```
Safety Score = (Hazards Detected / Total Hazards) Ã— 0.7 + 
               (1 - False Positive Rate) Ã— 0.3
```

**Approach Ranking by Safety:**
- Which approach is safest for navigation?
- Which approach has most false negatives (dangerous)?
- Which approach has most false positives (annoying but safe)?

**Visualization:**
- Safety score bar chart by approach
- False negative breakdown by hazard type
- Heatmap: Approach Ã— Hazard Type (detection rate)

---

#### **Category-Specific Performance Analysis** ğŸ¯ USE CASE FOCUS

**Gaming Scenario Deep Dive:**
- Which approach best identifies game UI elements?
- Which approach best describes character positions?
- Which approach best explains game mechanics?
- Latency requirements for gaming (can be slower than navigation)
- Quality vs speed tradeoff for gaming

**Indoor Navigation Deep Dive:**
- Which approach best detects obstacles?
- Which approach best describes spatial layout?
- Which approach has lowest false negative rate for hazards?
- Speed is important (real-time navigation)

**Outdoor Navigation Deep Dive:**
- Which approach best identifies safety-critical elements (crosswalks, vehicles)?
- Which approach handles varying lighting conditions?
- Which approach best describes distances?
- Safety is paramount (outdoor hazards more dangerous)

**Text Reading Deep Dive:**
- Which approach has highest OCR accuracy?
- Which approach best handles varied fonts/sizes?
- Which approach best interprets sign meaning (not just text)?
- Accuracy > speed for text reading

**Category Winner Matrix:**
| Category | Best Approach | Runner-up | Why? |
|----------|---------------|-----------|------|
| Gaming | - | - | - |
| Indoor Nav | - | - | - |
| Outdoor Nav | - | - | - |
| Text Reading | - | - | - |

**Visualization:**
- Category-specific performance radar charts
- Side-by-side comparison: Approach performance by category
- Heatmap: Approach Ã— Category (performance score)

---

## ğŸ—‚ï¸ PROJECT STRUCTURE

```
scene-reader/
â”œâ”€â”€ README.md                          # Project overview and setup
â”œâ”€â”€ PROJECT.md                         # This file - comprehensive documentation
â”œâ”€â”€ SCHEDULE.md                        # Detailed timeline and tasks
â”œâ”€â”€ DATA_COLLECTION_GUIDE.md          # How to gather test images
â”‚
â”œâ”€â”€ proposal/
â”‚   â”œâ”€â”€ Sivakumar_Khando_SceneReader_OnePage.docx
â”‚   â””â”€â”€ Sivakumar_Khando_SceneReader_OnePage.pdf
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ gaming/                   # 10 game screenshots
â”‚   â”‚   â”œâ”€â”€ indoor/                   # 10 indoor navigation scenes
â”‚   â”‚   â”œâ”€â”€ outdoor/                  # 10 outdoor navigation scenes
â”‚   â”‚   â””â”€â”€ text/                     # 10 text/sign images
â”‚   â”œâ”€â”€ ground_truth.csv              # Labeled test data
â”‚   â””â”€â”€ sources.txt                   # Image source documentation
â”‚
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ vlm_testing/                  # Approach 1: Pure VLMs
â”‚   â”‚   â”œâ”€â”€ test_api.py              # Unified VLM testing
â”‚   â”‚   â”œâ”€â”€ batch_test_all_models.py # Batch testing script
â”‚   â”‚   â”œâ”€â”€ prompts.py               # System prompts
â”‚   â”‚   â””â”€â”€ retest_failed.py         # Retry failed tests
â”‚   â”‚
â”‚   â”œâ”€â”€ approach_2_yolo_llm/          # Approach 2: Hybrid Pipeline âœ… COMPLETE
â”‚   â”‚   â”œâ”€â”€ yolo_detector.py         # YOLO object detection
â”‚   â”‚   â”œâ”€â”€ llm_generator.py         # LLM description generation
â”‚   â”‚   â”œâ”€â”€ hybrid_pipeline.py       # Full pipeline integration
â”‚   â”‚   â”œâ”€â”€ batch_test_yolo_llm.py   # Batch testing script
â”‚   â”‚   â”œâ”€â”€ prompts.py               # Prompt templates
â”‚   â”‚   â””â”€â”€ README.md                # Documentation
â”‚   â”‚
â”‚   â”œâ”€â”€ approach_3_specialized/       # Approach 3: Specialized
â”‚   â”‚   â”œâ”€â”€ ocr_enhanced.py          # OCR-enhanced system
â”‚   â”‚   â”œâ”€â”€ depth_enhanced.py        # Depth-enhanced system
â”‚   â”‚   â””â”€â”€ full_specialized.py      # Combined pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ approach_4_local/             # Approach 4: Local Models
â”‚   â”‚   â”œâ”€â”€ blip2_model.py           # BLIP-2 implementation
â”‚   â”‚   â”œâ”€â”€ batch_test_local.py     # Batch testing script
â”‚   â”‚   â””â”€â”€ local_vlm.py             # Local model utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ approach_5_streaming/         # Approach 5: Streaming
â”‚   â”‚   â”œâ”€â”€ streaming_pipeline.py    # Two-tier streaming
â”‚   â”‚   â””â”€â”€ async_handler.py         # Async implementation
â”‚   â”‚
â”‚   â”œâ”€â”€ approach_6_rag/               # Approach 6: RAG-Enhanced
â”‚   â”‚   â”œâ”€â”€ knowledge_base_builder.py # Build game wiki KB
â”‚   â”‚   â”œâ”€â”€ retriever.py             # Vector retrieval
â”‚   â”‚   â””â”€â”€ rag_pipeline.py          # RAG pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ approach_7_cot/               # Approach 7: Chain-of-Thought
â”‚   â”‚   â”œâ”€â”€ cot_prompts.py           # CoT prompt templates
â”‚   â”‚   â””â”€â”€ cot_tester.py            # CoT testing script
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ calculate_metrics.py     # Quantitative analysis
â”‚   â”‚   â”œâ”€â”€ manual_evaluation.py     # Qualitative scoring interface
â”‚   â”‚   â”œâ”€â”€ failure_analysis.py      # Categorize and analyze failures
â”‚   â”‚   â”œâ”€â”€ statistical_tests.py     # Statistical significance testing
â”‚   â”‚   â”œâ”€â”€ tradeoff_analysis.py     # Latency vs accuracy, cost vs quality
â”‚   â”‚   â”œâ”€â”€ safety_analysis.py       # Safety-critical error deep dive
â”‚   â”‚   â””â”€â”€ category_analysis.py     # Category-specific performance
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ image_loader.py          # Image handling utilities
â”‚       â”œâ”€â”€ timer.py                 # Latency measurement
â”‚       â””â”€â”€ cost_calculator.py       # API cost tracking
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ raw/                          # Raw outputs from all approaches
â”‚   â”‚   â”œâ”€â”€ batch_results.csv        # All VLM results (COMPLETE)
â”‚   â”‚   â”œâ”€â”€ approach_2_yolo_n.csv
â”‚   â”‚   â”œâ”€â”€ approach_2_yolo_m.csv
â”‚   â”‚   â”œâ”€â”€ approach_3_ocr.csv
â”‚   â”‚   â”œâ”€â”€ approach_3_depth.csv
â”‚   â”‚   â”œâ”€â”€ approach_4_blip2.csv
â”‚   â”‚   â”œâ”€â”€ approach_5_streaming.csv
â”‚   â”‚   â”œâ”€â”€ approach_6_rag.csv
â”‚   â”‚   â””â”€â”€ approach_7_cot.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/                     # Analysis notebooks
â”‚   â”‚   â”œâ”€â”€ 01_latency_analysis.ipynb
â”‚   â”‚   â”œâ”€â”€ 02_accuracy_analysis.ipynb
â”‚   â”‚   â”œâ”€â”€ 03_cost_analysis.ipynb
â”‚   â”‚   â”œâ”€â”€ 04_response_length_analysis.ipynb
â”‚   â”‚   â”œâ”€â”€ 05_tradeoff_analysis.ipynb
â”‚   â”‚   â”œâ”€â”€ 06_safety_analysis.ipynb
â”‚   â”‚   â”œâ”€â”€ 07_category_analysis.ipynb
â”‚   â”‚   â”œâ”€â”€ 08_failure_modes.ipynb
â”‚   â”‚   â””â”€â”€ 09_statistical_tests.ipynb
â”‚   â”‚
â”‚   â””â”€â”€ figures/                      # All visualizations
â”‚       â”œâ”€â”€ latency_comparison.png
â”‚       â”œâ”€â”€ accuracy_by_scenario.png
â”‚       â”œâ”€â”€ response_length_comparison.png
â”‚       â”œâ”€â”€ latency_vs_accuracy_tradeoff.png
â”‚       â”œâ”€â”€ cost_vs_quality_tradeoff.png
â”‚       â”œâ”€â”€ safety_score_comparison.png
â”‚       â”œâ”€â”€ category_performance_heatmap.png
â”‚       â”œâ”€â”€ failure_mode_breakdown.png
â”‚       â”œâ”€â”€ use_case_matrix.png
â”‚       â””â”€â”€ performance_radar.png
â”‚
â”œâ”€â”€ demo/                             # OPTIONAL - simple version OK
â”‚   â”œâ”€â”€ app.py                       # Simple Gradio/Streamlit demo
â”‚   â”œâ”€â”€ examples/                    # Pre-loaded example images
â”‚   â””â”€â”€ demo_video.mp4              # Screen recording backup
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ deployment_guide.md          # Recommendations for developers
â”‚   â”œâ”€â”€ implementation_guide.md      # How to integrate approaches
â”‚   â””â”€â”€ api_setup.md                 # API account setup instructions
â”‚
â”œâ”€â”€ report/
â”‚   â”œâ”€â”€ final_report.pdf             # Main deliverable
â”‚   â”œâ”€â”€ sections/                    # Individual report sections
â”‚   â””â”€â”€ appendices/                  # Additional materials
â”‚
â””â”€â”€ presentation/
    â”œâ”€â”€ slides.pptx                  # Presentation slides
    â”œâ”€â”€ slides.pdf                   # PDF version
    â””â”€â”€ script.md                    # Presentation notes
```

---

## ğŸ› ï¸ TECHNICAL STACK

### Required Tools & Libraries

#### **Python Environment**
```bash
# Python 3.10 or higher
python --version

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

#### **Core Dependencies**
```bash
pip install openai                    # GPT-4V API
pip install google-generativeai       # Gemini API
pip install anthropic                 # Claude API
pip install pillow                    # Image processing
pip install pandas                    # Data analysis
pip install numpy                     # Numerical operations
pip install matplotlib seaborn        # Visualization
pip install jupyter                   # Analysis notebooks
pip install python-dotenv             # Environment variables
pip install requests                  # HTTP requests
```

#### **Additional Dependencies (for other approaches)**
```bash
# Object Detection (Approach 2)
pip install ultralytics              # YOLOv8
pip install opencv-python            # Computer vision
pip install torch torchvision        # PyTorch (for YOLO)

# OCR (Approach 3)
pip install easyocr                  # Multi-language OCR
# OR
pip install paddleocr                # Alternative OCR

# Depth Estimation (Approach 3)
pip install transformers             # Hugging Face models
# For Depth-Anything, MiDaS

# Local Models (Approach 4)
pip install transformers             # BLIP-2 via Hugging Face

# RAG (Approach 6)
pip install chromadb                 # Vector database
pip install sentence-transformers    # Embeddings
pip install langchain                # RAG framework (optional)

# Analysis & Visualization
pip install scipy                    # Statistical tests
pip install plotly                   # Interactive plots
pip install seaborn                  # Statistical visualization
```

#### **Demo Dependencies (if time allows)**
```bash
pip install gradio                   # Interactive demos
# OR
pip install streamlit                # Alternative demo framework
```

---

### API Setup

#### **OpenAI (GPT-4V)**
1. Create account: https://platform.openai.com/
2. Add payment method (required for GPT-4V)
3. Generate API key: https://platform.openai.com/api-keys
4. Set environment variable:
   ```bash
   export OPENAI_API_KEY="sk-..."
   ```

**Costs:**
- GPT-4V: $0.01 per 1K input tokens + $0.03 per 1K output tokens
- Image: ~$0.03-0.08 per image depending on resolution

---

#### **Google (Gemini)**
1. Go to: https://aistudio.google.com/
2. Click "Get API Key"
3. Create new API key
4. Set environment variable:
   ```bash
   export GOOGLE_API_KEY="..."
   ```

**Costs:**
- Gemini 1.5 Pro: $0.00125 per 1K input tokens
- Generous free tier: 50 requests/day

---

#### **Anthropic (Claude)**
1. Create account: https://console.anthropic.com/
2. Add credits (minimum $5)
3. Generate API key
4. Set environment variable:
   ```bash
   export ANTHROPIC_API_KEY="sk-ant-..."
   ```

**Costs:**
- Claude 3.5 Sonnet: $0.003 per 1K input tokens
- Images: ~$0.024 per image (1.15M pixels)

---

### Environment Variables Setup

Create `.env` file in project root:
```bash
# API Keys
OPENAI_API_KEY=sk-...
GOOGLE_API_KEY=...
ANTHROPIC_API_KEY=sk-ant-...

# Project Configuration
PROJECT_NAME=scene-reader
DATA_DIR=./data
RESULTS_DIR=./results
```

**Load in Python:**
```python
from dotenv import load_dotenv
import os

load_dotenv()
openai_key = os.getenv('OPENAI_API_KEY')
```

---

## ğŸ“ DATA SPECIFICATION

### Image Requirements

#### **Technical Specifications**
- **Format:** PNG (preferred) or JPG (>90% quality)
- **Resolution:** Minimum 640x480, recommended 1280x720 or higher
- **Color space:** RGB (not grayscale)
- **File size:** <10MB per image
- **Lighting:** Well-lit, no extreme darkness or overexposure

#### **Content Requirements**
- **Clear subject:** Main elements in focus
- **Contextual:** Representative of real-world use cases
- **Diverse:** Variety within each category
- **Ethical:** No personal information, blurred faces if present

---

### Ground Truth Format

**File:** `data/ground_truth.csv`

**Columns:**
- `filename`: Image filename (e.g., "game_hollowknight_nav_01.png")
- `category`: gaming | indoor | outdoor | text
- `subcategory`: Specific type (e.g., "combat", "hallway", "crosswalk")
- `key_objects`: Comma-separated list of important objects
- `safety_critical`: Comma-separated list of hazards/obstacles
- `spatial_layout`: Text description of spatial relationships
- `text_content`: Actual text in image (if category=text)
- `description_requirements`: What a good description must include
- `difficulty`: easy | medium | hard

**Example Entry:**
```csv
filename,category,subcategory,key_objects,safety_critical,spatial_layout,text_content,description_requirements,difficulty
"indoor_hallway_01.png","indoor","hallway","hallway,door,person,trash_can","trash_can","Door at end of hallway (center), person walking toward camera (left side), trash can on right side","","Must mention door location, person position, trash can as obstacle",medium
```

---

## ğŸ§ª TESTING PROTOCOL

### Phase 1: API Validation (Day 1)
**Goal:** Ensure all APIs work correctly

**Steps:**
1. Test each API with 1 sample image
2. Verify response format
3. Check latency is reasonable
4. Confirm API keys are working
5. Test error handling

**Success Criteria:**
- All 3 APIs return descriptions
- No authentication errors
- Latency <10 seconds

---

### Phase 2: Baseline Testing (Days 2-4)
**Goal:** Run all images through all VLMs

**Process:**
```python
for each image in dataset:
    for each model in [gpt4v, gemini, claude]:
        start_time = record_time()
        description = model.describe(image)
        latency = calculate_latency(start_time)
        
        save_result(
            image=image,
            model=model,
            description=description,
            latency=latency,
            timestamp=now()
        )
        
        wait(1 second)  # Rate limiting
```

**Success Criteria:**
- All 40 images Ã— 3 models = 120 descriptions generated
- All latencies recorded
- All results saved to CSV

---

### Phase 3: Manual Evaluation (Days 5-6)
**Goal:** Qualitatively assess description quality

**Process:**
1. Load ground truth for image
2. Read VLM description
3. Score on 5 dimensions (1-5 scale each)
4. Note specific failures
5. Categorize failure type if applicable

**Division of Labor:**
- Each person evaluates 60 descriptions (20 images Ã— 3 models)
- Cross-check 10 images for consistency

---

### Phase 4: Analysis (Days 7-10)
**Goal:** Calculate all metrics and create visualizations

**Quantitative:**
- Calculate mean, median, p95, p99 latency for each model
- Calculate accuracy metrics from manual scores
- Estimate costs based on usage
- Create comparison charts

**Qualitative:**
- Aggregate failure modes
- Identify patterns (which model fails how?)
- Document safety-critical failures
- Synthesize insights

---

## ğŸ“ˆ EXPECTED OUTCOMES

### Hypothesis: VLM Performance

**GPT-4V:**
- Highest accuracy (90-95% object detection)
- Slowest latency (3-5 seconds)
- Most expensive (~$0.05 per query)
- Best at complex scenes and context

**Gemini 1.5 Pro:**
- Good accuracy (85-90%)
- Medium latency (2-4 seconds)
- Medium cost (~$0.03 per query)
- Strong at diverse scenarios

**Claude 3.5 Sonnet:**
- Good accuracy (85-90%)
- Medium latency (2-4 seconds)
- Medium cost (~$0.04 per query)
- Excellent at safety-critical details

---

### Use Case Recommendations (Predicted)

**For Gaming Accessibility:**
â†’ **GPT-4V** (accuracy matters more than speed)

**For Real-Time Navigation:**
â†’ **Gemini or Claude** (balance speed and accuracy)

**For Text Reading:**
â†’ **Any VLM** (all perform well on text)

**For Cost-Sensitive Applications:**
â†’ **Gemini** (best price-performance ratio)

---

## ğŸš§ KNOWN LIMITATIONS

### Technical Limitations

**Latency Constraint:**
- All VLMs currently 2-5 seconds
- True real-time (<500ms) not achievable with current models
- "Near real-time" is the best we can offer

**Accuracy Challenges:**
- Hallucinations inevitable with generative models
- Spatial relationships sometimes incorrect
- Small objects may be missed

**Cost Barriers:**
- Commercial API costs add up for heavy usage
- Free tiers insufficient for production deployment

---

### Scope Limitations

**What We're NOT Doing:**
- Building production-ready system
- User testing with blind individuals
- Real-time video processing (only static images)
- Audio integration (text-to-speech)
- Mobile app development
- Custom model training

**What We ARE Doing:**
- Systematic comparison of existing approaches
- Feasibility analysis for deployment
- Recommendations for developers
- Documentation of capabilities and limitations

---

### Dataset Limitations

**Size:** 40 images (not thousands)
- Sufficient for comparison
- Not sufficient for training
- Focused on breadth over depth

**Diversity:** Limited scenarios
- 4 categories covered
- Missing: medical, workplace, transportation
- Generalizations must be cautious

**Ground Truth:** Manual labeling
- Subjective elements exist
- Inter-rater reliability not formally measured
- Assumes our labels are correct

---

## ğŸ“š RELATED WORK & REFERENCES

### Key Papers

**Vision-Language Models:**
- "CLIP: Learning Transferable Visual Models From Natural Language Supervision" (Radford et al., 2021)
- "Flamingo: a Visual Language Model for Few-Shot Learning" (Alayrac et al., 2022)
- "GPT-4V(ision) System Card" (OpenAI, 2023)

**Object Detection:**
- "You Only Look Once: Unified, Real-Time Object Detection" (Redmon et al., 2016)
- "DETR: End-to-End Object Detection with Transformers" (Carion et al., 2020)
- "YOLOv8: Pushing the Boundaries of Real-Time Object Detection" (Ultralytics, 2023)

**Accessibility Applications:**
- "VizWiz: Nearly Real-Time Answers to Visual Questions" (Bigham et al., 2010)
- "Seeing AI: A Research Platform for Understanding Human-AI Interaction" (Microsoft, 2017)

---

### Existing Accessibility Tools

**Microsoft Seeing AI:**
- Mobile app for blind users
- Scene description, text reading, person recognition
- Not real-time, not gaming-focused

**Be My Eyes:**
- Human volunteer assistance platform
- Recently integrated GPT-4V
- High latency (requires human response)

**Envision AI:**
- Mobile app for text reading and scene understanding
- Limited gaming support
- Commercial product

**Our Contribution:**
- First systematic comparison of approaches
- Gaming accessibility focus (underexplored)
- Explicit tradeoff analysis for developers

---

## ğŸ“ LEARNING OBJECTIVES ALIGNMENT

### Course Goals Met

**From DS-5690 Syllabus:**

âœ… **"Speak knowledgeably about generative AI, how it works"**
- Deep dive into vision-language transformers
- Understanding of multimodal architectures
- Practical experience with state-of-the-art models

âœ… **"Describe succinctly architectures, algorithms and approaches"**
- Comparison of VLM architectures
- Analysis of vision transformers (ViT)
- Cross-modal attention mechanisms

âœ… **"Quickly ascertain abilities and limitations of new models"**
- Systematic testing methodology
- Failure mode identification
- Performance boundary documentation

âœ… **"Understand technical tradeoffs in deployed systems"**
- Latency vs. accuracy analysis
- Cost vs. performance tradeoffs
- Cloud vs. edge deployment considerations

âœ… **"Demonstrate mastery through implementation and analysis"**
- Working implementations of multiple approaches
- Rigorous quantitative and qualitative evaluation
- Actionable recommendations for practitioners

---

## ğŸ¤ TEAM RESPONSIBILITIES

### Roshan Sivakumar

**Approaches Owned:**
- âœ… Approach 1: Pure VLMs (all 3 models) - COMPLETE
- ğŸ”¨ Approach 7: Chain-of-Thought (prompt engineering) - TIER 1
- â­ Approach 5: Streaming (if time allows) - TIER 2

**Analysis Responsibilities:**
- Latency analysis across all approaches
- Cost analysis and projections
- Statistical significance testing
- Visualization creation (charts, plots)

**Report Sections:**
- Introduction (3 pages)
- Methodology: Approaches 1, 5, 7 (4 pages)
- Results: Quantitative Analysis (5 pages)
- Novel Findings: Streaming & CoT (2 pages)

**Estimated Time:** 40-50 hours over 3 weeks

---

### Dhesel Khando

**Approaches Owned:**
- ğŸ”¨ Approach 2: YOLO + LLM (priority) - TIER 1
- â­ Approach 4: Local Models (BLIP-2) - TIER 2
- âœ… Approach 3: Specialized (OCR/Depth) - COMPLETE - TIER 3
- ğŸ”¨ Approach 6: RAG-Enhanced (if time) - TIER 1

**Analysis Responsibilities:**
- Accuracy evaluation (manual scoring)
- Failure mode categorization
- Use case suitability analysis
- Cost vs performance tradeoffs

**Report Sections:**
- Related Work (2 pages)
- Methodology: Approaches 2, 3, 4, 6 (6 pages)
- Results: Qualitative Analysis (4 pages)
- Novel Findings: RAG for Gaming (2 pages)

**Estimated Time:** 40-50 hours over 3 weeks

---

### Joint Responsibilities

**Implementation:**
- Ground truth labeling (20 images each)
- Code review and debugging
- Integration testing
- Demo development

**Analysis:**
- Manual evaluation sessions (split descriptions)
- Deployment recommendations
- Use case matrix creation

**Documentation:**
- Report editing and integration
- Presentation creation
- Demo video recording
- Code documentation

**Meetings:**
- 2x per week check-ins (1 hour each)
- Daily async updates via Slack
- Final presentation practice (2 hours)

---

## ğŸ“ COMMUNICATION PROTOCOL

### Regular Check-ins
**Daily (Nov 19-26):**
- Morning: 5-min Slack message (progress, blockers)
- Evening: Quick status update

**Meetings:**
- Nov 19 evening: Kickoff + image review
- Nov 22: Mid-week check-in (1 hour)
- Nov 25: Week 1 wrap-up (1 hour)
- Nov 27: Analysis planning (1 hour)
- Dec 1: Report review (2 hours)
- Dec 3: Presentation practice (1 hour)

### Escalation Protocol
**If Stuck (>2 hours):**
1. Google/ChatGPT for solutions (30 min)
2. Message partner
3. Schedule call if needed

**If Critical Blocker:**
- Immediately notify partner
- Assess scope adjustment
- No suffering in silence!

---

## âœ… SUBMISSION CHECKLIST

### Code Deliverables
- [ ] All source code committed to repository
- [ ] Code is documented with comments
- [ ] README.md with setup instructions
- [ ] requirements.txt with all dependencies
- [ ] .env.example showing required variables

### Data Deliverables
- [ ] 40 test images in organized folders
- [ ] ground_truth.csv with all labels
- [ ] sources.txt documenting image origins
- [ ] Sample images for demo

### Results Deliverables
- [ ] All raw results CSVs (one per model)
- [ ] Analysis notebooks (executed with outputs)
- [ ] All figures and visualizations (PNG/PDF)
- [ ] Summary statistics tables

### Documentation Deliverables
- [ ] Final report (PDF, 15-20 pages)
- [ ] Deployment recommendations guide
- [ ] Implementation guide for developers
- [ ] PROJECT.md (this file)

### Presentation Deliverables
- [ ] Presentation slides (PPTX + PDF)
- [ ] Presenter notes/script
- [ ] Demo video (backup if live demo fails)
- [ ] Example outputs to show

---

## ğŸ¯ SUCCESS CRITERIA

### Minimum Viable Project (B Grade)
- âœ… 2 VLMs tested successfully
- âœ… 30+ images in dataset
- âœ… Basic latency and accuracy metrics
- âœ… Simple comparative analysis
- âœ… Clear presentation

### Target Project (A- to A Grade)
- âœ… 3 VLMs tested comprehensively
- âœ… 40 images across all scenarios
- âœ… Comprehensive quantitative metrics
- âœ… Detailed qualitative analysis
- âœ… Failure mode categorization
- âœ… Deployment recommendations
- âœ… Professional report and presentation

### Exceptional Project (A+ Grade)
- âœ… All of above +
- âœ… Working object detection comparison
- âœ… Interactive demo
- âœ… Novel insights or findings
- âœ… Publication-quality documentation
- âœ… Submission to AI Showcase

---

## ğŸš€ GETTING STARTED

### Day 1 Quick Start (Nov 19)

**Morning (Both):**
```bash
# 1. Set up project structure
mkdir scene-reader
cd scene-reader
mkdir -p data/images/{gaming,indoor,outdoor,text}
mkdir -p code results docs

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate

# 3. Install core dependencies
pip install openai google-generativeai anthropic pillow pandas

# 4. Set up .env file
touch .env
# Add your API keys to .env
```

**Afternoon (Image Collection):**
- Roshan: Gaming + Indoor
- Dhesel: Outdoor + Text
- Target: 40 images by end of day

**Evening (Meeting):**
- Upload images to shared Google Drive
- Create initial ground_truth.csv
- Test API access
- Plan tomorrow's tasks

---

## ğŸ“– ADDITIONAL RESOURCES

### Tutorials & Documentation
- **OpenAI Vision API:** https://platform.openai.com/docs/guides/vision
- **Gemini API:** https://ai.google.dev/tutorials/python_quickstart
- **Claude Vision:** https://docs.anthropic.com/claude/docs/vision
- **YOLOv8:** https://docs.ultralytics.com/

### Datasets for Reference
- **VizWiz:** https://vizwiz.org/
- **COCO:** https://cocodataset.org/
- **ImageNet:** https://www.image-net.org/

### Tools
- **Gradio:** https://www.gradio.app/ (for demos)
- **Weights & Biases:** https://wandb.ai/ (experiment tracking)
- **Jupyter:** https://jupyter.org/ (analysis)

---

## ğŸ“„ LICENSE & ATTRIBUTION

### Project License
This project is for academic purposes only.
- Code: MIT License (if open-sourced later)
- Documentation: CC BY 4.0
- Dataset: Not for redistribution (contains third-party images)

### Image Attribution
All image sources documented in `data/sources.txt`
- Personal photos: Original work
- Stock photos: Licensed for academic use (Unsplash/Pexels)
- Game screenshots: Fair use for academic research
- Dataset images: Cited with original source

### Model Attribution
- GPT-4V: OpenAI (https://openai.com/)
- Gemini: Google DeepMind (https://deepmind.google/)
- Claude: Anthropic (https://www.anthropic.com/)

---

## ğŸ“® CONTACT & SUPPORT

### Team Contact
- **Roshan Sivakumar:** roshan.sivakumar@vanderbilt.edu
- **Dhesel Khando:** dhesel.khando@vanderbilt.edu

### Course Staff
- **Instructor:** Jesse Spencer-Smith (jesse.spencer-smith@vanderbilt.edu)
  - Office Hours: Fridays 8am-9am
  - AI Friday Hours: 10am-12pm, Data Science Team Space

- **TA:** Shivam Tyagi (shivam.tyagi@vanderbilt.edu)
  - Office Hours: Tuesday/Thursday 3pm-4pm
  - Location: DSI Common Room

### Emergency Contacts
- DSI Help: dsi@vanderbilt.edu
- Technical Issues: Course Slack channel

---

## ğŸ‰ ACKNOWLEDGMENTS

### Special Thanks
- Professor Jesse Spencer-Smith for project guidance
- Shivam Tyagi for technical support
- Vanderbilt Data Science Institute for resources
- Open-source community for tools and libraries

### Inspiration
This project was inspired by the need to make visual information accessible to the 7+ million blind and visually impaired individuals in the US, and the 250+ million worldwide.

---

## ğŸ“… VERSION HISTORY

**v1.0 (Nov 18, 2025)** - Initial project setup
- Project scope defined
- Team assigned
- Documentation created

**v1.1 (Nov 19, 2025)** - Data collection phase
- Images collected
- Ground truth begun
- APIs tested

**v2.0 (Nov 22, 2025)** - Comprehensive update
- Added 7 approaches (VLMs, YOLO+LLM, Specialized, Local, Streaming, RAG, CoT)
- Updated with novel contributions
- Enhanced technical details
- Updated team responsibilities
- Added comprehensive comparison matrix

**v2.1 (Dec 4, 2025)** - Project completion (target)
- All testing complete
- Final report submitted
- Presentation delivered

---

## ğŸ”® FUTURE WORK

### Potential Extensions
1. **User Study:** Test with actual blind users
2. **Real-time Video:** Extend to video streams
3. **Mobile App:** Develop smartphone application
4. **Custom Training:** Fine-tune models on accessibility data
5. **Multimodal Output:** Add audio descriptions and haptic feedback
6. **Cost Optimization:** Implement intelligent caching and batching
7. **Edge Deployment:** Port to mobile/embedded devices
8. **Additional Scenarios:** Medical, workplace, transportation contexts

---

## ğŸ“Š PROJECT STATUS

**Current Status:** ğŸŸ¢ Phase 1 Complete + Speed Optimization In Progress - Ready for Phase 2 Implementation

**Methodology:** Two-Phase Approach
- âœ… **Phase 1: Comprehensive Evaluation** - COMPLETE
  - Static image testing (42 images across 4 scenarios)
  - 5 approaches tested and compared
  - Optimal solutions identified based on **latency**, accuracy, and cost
- ğŸ”„ **Speed Optimization** - IN PROGRESS
  - Testing faster LLM models (GPT-3.5-turbo, Gemini Flash) in Approach 2
  - **Target:** Achieve <2 second latency for real-time use
  - **Current Best:** 3.73s mean (YOLO+LLM), targeting <2s with faster LLMs
- ğŸ”„ **Phase 2: Real-Time Implementation** - Ready to begin
  - Selected model/approach from Phase 1 (prioritizing speed)
  - Video capture and TTS integration for live gaming
  - **Focus:** <2 second latency for real-time gaming

**Progress Tracking:**
```
âœ… Week 1: Data Collection (COMPLETE)
   - 42 images collected
   - Ground truth labeled
   - APIs tested

âœ… Week 2: Baseline VLM Testing (COMPLETE)
   - Approach 1: VLMs (DONE - 126 API calls)
   - Universal system prompt implemented
   - All results saved to batch_results.csv

âœ… Week 2-3: Core Approaches (COMPLETE)
   - âœ… Approach 2: YOLO+LLM (COMPLETE - Tested, analyzed, documented)
   - âœ… Approach 7: Chain-of-Thought (COMPLETE - Tested, analyzed, documented)
   - âœ… Approach 6: RAG-Enhanced (COMPLETE - Tested, analyzed, documented)
   - âœ… Approach 4: Local Models (COMPLETE - Tested, analyzed, documented)
   
âœ… Week 3-4: Speed Optimization (COMPLETE)
   - âœ… Approach 2.5: Optimized YOLO+LLM (COMPLETE - 1.10s mean, <2s target achieved)
   - âœ… Approach 3.5: Optimized Specialized (COMPLETE - 1.50s mean, 75% under 2s)

âœ… Week 3-4: Advanced Approaches (COMPLETE)
   - âœ… Approach 5: Streaming (COMPLETE - 1.73s perceived latency, 69% improvement)
   - ğŸ”„ Approach 3: Specialized (if time)

ğŸ“Š Week 4: Analysis & Deliverables (IN PROGRESS)
   - âœ… Comprehensive comparison complete
   - âœ… Statistical analysis complete
   - âœ… Latency comparison documented
   - ğŸ”„ Final report writing
   - ğŸ”„ Deployment recommendations
   - ğŸ”„ Presentation creation

ğŸ¯ Dec 4: SUBMISSION DEADLINE
```

**Phase 1 Findings:**
- **Fastest Approach:** Approach 2.5 - Optimized YOLO+LLM (1.10s mean latency, **<2s target achieved**)
- **Best Perceived Latency:** Approach 5 - Streaming (1.73s time to first output, **69% improvement**)
- **Speed Improvement:** 67.4% faster than Approach 2 baseline (3.39s â†’ 1.10s)
- **Perceived Latency Improvement:** 69% faster perceived response (1.73s vs 5.63s baseline)
- **Optimizations:** GPT-3.5-turbo model, caching (15x speedup), adaptive parameters, progressive disclosure
- **Statistical Significance:** Highly significant improvement (p < 0.000001, Cohen's d = 2.61)
- **Best for Gaming:** RAG-Enhanced (educational context), Approach 2.5 (speed-critical), Approach 5 (perceived speed)
- **Most Consistent:** Claude 3.5 Haiku (0.99s std dev)
- **Zero Cost Option:** Local Models (35.4s latency, $0.00 cost)
- **UX Innovation:** Approach 5 provides immediate feedback (1.73s) while detailed description generates (5.47s)

**Last Updated:** November 25, 2025

---

**End of PROJECT.md**

*This document is a living reference and will be updated throughout the project lifecycle.*
